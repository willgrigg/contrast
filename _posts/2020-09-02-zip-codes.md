---
title:  "The clunkiest way to web-scrape zip code information"
date:   2020-09-02
---

During my endless quest to own property, I have stumbled upon <a href="https://www.zip-codes.com">a little website</a> that lists information about areas defined by zip codes (property values, population, etc.)

Of course, these guys are trying to sell something -- namely, a database that contains all this information in a much more easily accessible format. However, all I'm really interested in is the <u><strong>average house value</strong></u> field for each zip code, and I'm not particularly interested in paying any money for information that is publicly available.

I've already downloaded the information web page for every zip code in the United States from this website using this very clunky command:

```
for i in {00501..99950}; do curl -o ${i}.html https://www.zip-codes.com/zip-code/${i}/zip-code-${i}.asp; done
```

<em>(00501 is the lowest zip code in the United States and 99950 is the highest.)</em>

This takes a little while, but it essentially downloads every piece of relevant information on this site. This will also unfortunately generate a lot of files that will have no data (because there is no corresponding zip code.) To remove these, use this command:

```
find -size 0 -delete
```

Once you do that, you'll be left with zip codes that are actually in use (roughly 42,000).

Alright, so we have a bunch of goddamn .html files now -- not exactly the best situation. What do we do from here? Well, as I mentioned before, all *I'm* personally interested in are the <u><strong>average house values</strong></u> for each zip code, which I think is a good place to start. If I can find a way to read every .html file in order and add their zip code/property value fields to a .csv file, I'm good to go.

<em>(This sort of requires us to make a pretty big assumption: that all these .html files are formatted in essentially the exact same way. This is an assumption I am willing to make.)</em>

Let's take a look at one of the .html files and see how the "average house value" field is formatted:

<strong>82001.html excerpt:</strong>


```
...long lines of text preceding...
>Average House Value:</span></td><td class="info">$184,400</td></tr>
...long lines of text following...
```

Excellent! We now have a string we can search for in each file: <strong>">Average House Value:</span></td><td class="info">(PROPERTY_VALUE)</td></tr>"</strong>. 

Now let's create a .csv file with a "zip" heading and an "avg_house_value" heading:

```
echo "zip,avg_house_value" >> zips.csv
```

OK, hold onto your ass -- here's the most dreadfully written script ever written:

```
for f in *.html; do file_content=`cat $f`; chunk=`echo "$file_content" | awk -F'>Average House Value:</span></td><td class="info">' '{print $2}'`; value=`echo "$chunk" | awk -F'</td></tr>'  '{print $1}'`; value=`echo $value | sed 's/,//g'`; echo "$(basename $f .html),$value" >> zips.csv; done

```

Jesus Christ. All this complexity for the sake of simplicity. I cannot even go over every element of this command, but suffice to say, it's what I've come up with on short notice and it does the job. Someone hire me
